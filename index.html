<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts">
  <meta name="keywords" content="Multi-modal 3D instruction tuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ch3cook-fdu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/ch3cook-fdu/Vote2Cap-DETR">
            Vote2Cap-DETR
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Mingsheng Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chenxin.tech/">Xin Chen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://icoz69.github.io/">Chi Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ch3cook-fdu.github.io/">Sijin Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hongyuanzhu.github.io/">Hongyuan Zhu</a><sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://fukunyin.github.io/">Fukun Yin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.skicyyu.org/">Gang Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://eetchen.github.io/">Tao Chen</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Tencent PCG,</span <br>
            <span class="author-block"><sup>3</sup>Institute for Infocomm Research (I2R) & Centre for Frontier AI Research (CFAR), A*STAR, Singapore</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/files/M3DBench.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
<!--                 <a href="https://arxiv.org/abs/2311.18651" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Benchmark Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-chart-bar"></i>
                  </span>
                  <span>Benchmark</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h6 class="subtitle has-text-centered">
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>M3DBench</b> introduces a comprehensive 3D instruction-following dataset that encompasses a variety of 3D vision-centric tasks, spanning fundamental abilities in real-world 3D environments. 
        <br><br>
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>M3DBench</b> supports multi-modal instructions interleaved with diverse visual prompts. 
        <br><br>
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>M3DBench</b> provides a new benchmark for assessing large models across 3D tasks.
<!--         ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>ViP-Bench</b> is the first zero-shot region level benchmark 
        for large multimodal models.  ViP-Bench not only includes the bounding box format, but also representsand f arbitrary visual prompts annotated by human.  -->
      </h6>
    </div>
  </div>
</section>
  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <centering>
            <div style="text-align: center;">
              <img id="teaser" width="97%" src="static/images/teaser.png">
            </div>
      </centering>
      <br>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="carousel-container" style="display: flex; justify-content: center;">
          <div id="results-carousel" class="carousel results-carousel" style="width: 100%;">
            <div class="item item-steve">
              <video poster="" id="steve" autoplay controls muted loop playsinline>
                <source src="./static/videos/7.3.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-chair-tp">
              <video poster="" id="chair-tp" autoplay controls muted loop playsinline>
                <source src="./static/videos/4a.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-chair">
              <video poster="" id="chair" autoplay controls muted loop playsinline>
                <source src="./static/videos/7.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-shiba">
              <video poster="" id="shiba" autoplay controls muted loop playsinline>
                <source src="./static/videos/7a.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br>
    <h6 class="subtitle has-text-centered">
      Examples from M3DBench, which encompasses a variety of 3D-centric tasks.
    </h6>
<!--     </div> -->
  </div>
</section>
  

<section class="hero is-white is-small">
  <div class="hero-body">
    <div class="container">
      <div class="carousel-container" style="display: flex; justify-content: center;">
        <div id="results-carousel" class="carousel results-carousel" style="width: 80%;">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline>
              <source src="./static/videos/4.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline>
              <source src="./static/videos/4a.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-chair">
            <video poster="" id="chair" autoplay controls muted loop playsinline>
              <source src="./static/videos/7.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline>
              <source src="./static/videos/7a.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recently, 3D understanding has become popular to facilitate autonomous agents to perform further decisionmaking. However, existing 3D datasets and methods are often limited to specific tasks. On the other hand, recent progress in Large Language Models (LLMs) and Multimodal Language Models (MLMs) have demonstrated exceptional general language and imagery tasking performance. 
            Therefore, it is interesting to unlock MLMâ€™s potential to be 3D generalist for wider tasks. 
            However, current MLMsâ€™ research has been less focused on 3D tasks due to a lack of large-scale 3D instruction-following datasets. 
            In this work, we introduce a comprehensive 3D instructionfollowing dataset called M3DBench, which possesses the following characteristics: 
            1) It supports <b>general multimodal instructions</b> interleaved with text, images, 3D objects, and other visual prompts. 
            2) It unifies <b>diverse 3D tasks at both region and scene levels</b>, covering a variety of <b>fundamental abilities</b> in real-world 3D environments.
            3) It is a large-scale 3D instruction-following dataset with <b>over 320k instruction-response pairs</b>. 
            Furthermore, we establish a new <b>benchmark</b> for assessing the performance of large models in understanding multi-modal 3D prompts.
            Extensive experiments demonstrate the effectiveness of our dataset and baseline, supporting general 3D-centric tasks, which can inspire future research.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/teaser_20231208_14160748.mp4"
                  type="video/mp4">
        </video>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/224JzkdHjfg?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <p>
            <b>Dataset Download:</b> If you are interested in M3DBench dataset, you can download it <a href="https://m3dbench.github.io/">here</a>. 
          </p>
        </div>
        
        <div class="content has-text-justified">
          <p>
           <b>Comparison between M3DBench and other 3D VL datasets as well as 3D instruction datasets.</b>
           M3DBench has the following characteristics: 1) A comprehensive instruction-following dataset tailored for 3D scenes. 
           2) Supporting multi-modal instructions that interleave text, coordinate, image, 3D object, and so on. 
           3) Encompassing diverse 3D visual-centric tasks that span a variety of fundamental abilities in real-world 3D environments, such as visual perception, scene understanding, spatial reasoning, navigation, and planning.
          </p>

        </div>
      </div>
    </div>
    <!--/ Animation. -->
    <centering>
      <div style="text-align: center;">
        <img id="pipeline" style="max-width: 100%;" src="static/images/comparison.png">
      </div>
    </centering>
    <br><br>
    
    <!-- Animation. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dataset</h2> -->
        <div class="content has-text-justified">
          <p>
           <b>The statistics of the M3DBench.</b>
            (a) The distribution of instructions based on the first word, where the inner circle of the graph represents the frequency of the first wordâ€™s occurrence, and the outer circle shows the frequency of verbs and nouns appearing in the instructions corresponding to that first word. 
            (b) The word cloud of responses. 
            (c) The distribution of instruction length. (d) The distribution of response length
          </p>

        </div>
<!--       </div>
    </div> -->
    <!--/ Animation. -->
    <centering>
      <div style="text-align: center;">
        <img id="pipeline" style="max-width: 100%;" src="static/images/statistics.png">
      </div>
    </centering>
  
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a baseline model that connects scenes with interleaved multi-modal instructions and accomplishes diverse tasks using a unified decoder. 
            Specifically, we utilize scene perceiver to extract scene tokens from 3D visual input. Multi-modal instructions are transformed into corresponding instruction tokens via their respective encoders. The scene tokens and multi-modal instruction tokens are then concatenated and fed into a frozen LLM, which generates the corresponding responses subsequently. During the training process, only the projectors are updated.
          </p>
          <centering>
            <div style="text-align: center;">
              <img id="pipeline" width="85%" src="static/images/pipeline.png">
            </div>
          </centering>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3">Related Links</h2>
        <div class="content has-text-justified">
          <p>
             There are also outstanding concurrent works, such as:
            <a href="https://arxiv.org/abs/2306.06687">LAMM</a>, 
            <a href="https://arxiv.org/abs/2307.12981">3D-LLM</a>, 
            <a href="https://arxiv.org/abs/2308.08769">Chat-3D</a>, 
            <a href="https://arxiv.org/abs/2309.00615">Point-Bind & Point-LLM</a>,
            <a href="http://arxiv.org/abs/2308.16911">PointLLM</a>,
            <a href="https://arxiv.org/abs/2311.12871">LEO</a>, and <a href="https://arxiv.org/abs/2311.18651">LL3DA</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>




<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{chen2023ll3da,
    title={LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning}, 
    author={Sijin Chen and Xin Chen and Chi Zhang and Mingsheng Li and Gang Yu and Hao Fei and Hongyuan Zhu and Jiayuan Fan and Tao Chen},
    year={2023},
    eprint={2311.18651},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/files/M3DBench.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/MSheng-Lee" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
